{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Helda05/Models/blob/main/ML_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGyWGR8UAS_Z",
        "outputId": "2b396d68-7c06-4fa4-d693-1d95fd337eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation results:\n",
            "                  model  accuracy        f1  precision    recall       mcc  \\\n",
            "0           Naive Bayes  0.805556  0.533333   0.500000  0.571429  0.412677   \n",
            "1           Naive Bayes  0.805556  0.533333   0.500000  0.571429  0.412677   \n",
            "2           Naive Bayes  0.694444  0.592593   0.500000  0.727273  0.377552   \n",
            "3           Naive Bayes  0.777778  0.636364   0.700000  0.583333  0.482382   \n",
            "4           Naive Bayes  0.611111  0.300000   0.333333  0.272727  0.034816   \n",
            "5           Naive Bayes  0.805556  0.666667   0.636364  0.700000  0.531050   \n",
            "6           Naive Bayes  0.805556  0.666667   0.875000  0.538462  0.571876   \n",
            "7           Naive Bayes  0.666667  0.454545   0.454545  0.454545  0.214545   \n",
            "8           Naive Bayes  0.885714  0.750000   0.666667  0.857143  0.686406   \n",
            "9           Naive Bayes  0.742857  0.640000   0.800000  0.533333  0.474693   \n",
            "10                  KNN  0.833333  0.250000   1.000000  0.142857  0.344046   \n",
            "11                  KNN  0.750000  0.470588   0.400000  0.571429  0.322105   \n",
            "12                  KNN  0.722222  0.444444   0.571429  0.363636  0.283570   \n",
            "13                  KNN  0.750000  0.571429   0.666667  0.500000  0.408248   \n",
            "14                  KNN  0.611111  0.300000   0.333333  0.272727  0.034816   \n",
            "15                  KNN  0.722222  0.375000   0.500000  0.300000  0.221880   \n",
            "16                  KNN  0.694444  0.352941   0.750000  0.230769  0.286251   \n",
            "17                  KNN  0.611111  0.300000   0.333333  0.272727  0.034816   \n",
            "18                  KNN  0.828571  0.625000   0.555556  0.714286  0.522976   \n",
            "19                  KNN  0.685714  0.476190   0.833333  0.333333  0.372035   \n",
            "20                  MLP  0.861111  0.545455   0.750000  0.428571  0.496292   \n",
            "21                  MLP  0.777778  0.428571   0.428571  0.428571  0.290640   \n",
            "22                  MLP  0.750000  0.526316   0.625000  0.454545  0.370679   \n",
            "23                  MLP  0.833333  0.700000   0.875000  0.583333  0.614192   \n",
            "24                  MLP  0.694444  0.476190   0.500000  0.454545  0.261785   \n",
            "25                  MLP  0.861111  0.782609   0.692308  0.900000  0.695792   \n",
            "26                  MLP  0.777778  0.555556   1.000000  0.384615  0.534191   \n",
            "27                  MLP  0.694444  0.421053   0.500000  0.363636  0.225630   \n",
            "28                  MLP  0.942857  0.857143   0.857143  0.857143  0.821429   \n",
            "29                  MLP  0.771429  0.692308   0.818182  0.600000  0.533002   \n",
            "30          KNN Average  0.720873  0.416559   0.594365  0.370176  0.283074   \n",
            "31          MLP Average  0.796429  0.598520   0.704620  0.545496  0.484363   \n",
            "32  Naive Bayes Average  0.760079  0.577350   0.596591  0.580967  0.419868   \n",
            "\n",
            "         auc  \n",
            "0   0.763547  \n",
            "1   0.566502  \n",
            "2   0.781818  \n",
            "3   0.822917  \n",
            "4   0.650909  \n",
            "5   0.838462  \n",
            "6   0.842809  \n",
            "7   0.807273  \n",
            "8   0.948980  \n",
            "9   0.780000  \n",
            "10  0.660099  \n",
            "11  0.645320  \n",
            "12  0.767273  \n",
            "13  0.798611  \n",
            "14  0.507273  \n",
            "15  0.596154  \n",
            "16  0.520067  \n",
            "17  0.418182  \n",
            "18  0.591837  \n",
            "19  0.568333  \n",
            "20  0.798030  \n",
            "21  0.704433  \n",
            "22  0.832727  \n",
            "23  0.899306  \n",
            "24  0.745455  \n",
            "25  0.846154  \n",
            "26  0.862876  \n",
            "27  0.810909  \n",
            "28  0.959184  \n",
            "29  0.883333  \n",
            "30  0.607315  \n",
            "31  0.834241  \n",
            "32  0.780322  \n",
            "Cross-validation results with averages saved to cv_results.xlsx\n",
            "Model predictions:\n",
            "         model                                        predictions\n",
            "0  Naive Bayes  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "1          KNN  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, ...\n",
            "2          MLP  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, ...\n",
            "Model predictions saved to test_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "abcd\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, df):\n",
        "        # Initialize the preprocessor with a DataFrame\n",
        "        self.df = df\n",
        "        self.numerical_features = ['F{}'.format(i) for i in range(1, 18)]\n",
        "        self.boolean_features = ['F{}'.format(i) for i in range(18, 78)]\n",
        "\n",
        "    def preprocess(self):\n",
        "        # Apply various preprocessing methods on the DataFrame\n",
        "        self.df = self._preprocess_numerical(self.df)\n",
        "        self.df = self._preprocess_categorical(self.df)\n",
        "        return self.df\n",
        "\n",
        "    def _preprocess_numerical(self, df):\n",
        "        # Custom logic for preprocessing numerical features goes here\n",
        "\n",
        "        # Impute missing values with median of each col\n",
        "        median = df[self.numerical_features].median()\n",
        "        df[self.numerical_features] = df[self.numerical_features].fillna(median)\n",
        "\n",
        "        for feature in self.numerical_features:\n",
        "          # Clip extreme values to 1st and 99th percentiles\n",
        "          percentiles = df[feature].quantile([0.01, 0.99]).values\n",
        "          df[feature] = df[feature].clip(percentiles[0], percentiles[1])\n",
        "\n",
        "\n",
        "          mean = df[feature].mean()\n",
        "          std_dev = df[feature].std()\n",
        "\n",
        "          if std_dev != 0:\n",
        "                # Z-score scaling\n",
        "              df[feature] = (df[feature] - mean) / std_dev\n",
        "          else:\n",
        "                # Set all values to a default value\n",
        "              df[feature] = 0\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def _preprocess_categorical(self, df):\n",
        "        # Add custom logic here for categorical features\n",
        "\n",
        "        #impute Nan with mode of each col\n",
        "        mode = df[self.boolean_features].mode()\n",
        "        df[self.boolean_features] = df[self.boolean_features].fillna(mode).iloc[0]\n",
        "        return df\n",
        "\n",
        "    def _preprocess_ordinal(self, df):\n",
        "        # Custom logic for preprocessing ordinal features goes here\n",
        "        return df\n",
        "\n",
        "# Base classifier class\n",
        "class Classifier(ABC):\n",
        "    @abstractmethod\n",
        "    def fit(self, X, y):\n",
        "        # Abstract method to fit the model with features X and target y\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        # Abstract method to make predictions on the dataset X\n",
        "        pass\n",
        "\n",
        "# K-Nearest Neighbors Classifier\n",
        "class KNearestNeighbors(Classifier):\n",
        "    def __init__(self, k=3):\n",
        "        # Initialize KNN with k neighbors\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Store training data and labels for KNN\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Implement the prediction logic for KNN\n",
        "        predictions = []\n",
        "        for pt in X.values:\n",
        "          distances = [np.sqrt(np.sum((x - pt)**2)) for x in self.X_train.values]\n",
        "          sorted_indices = np.argsort(distances)[:self.k] #sort in ascending + extract indices of first 3 neighbours\n",
        "          predicted_outcome = self.y_train.loc[sorted_indices] #extract predicted outcomes\n",
        "          #extract outcome with highest count\n",
        "          prediction = max(set(predicted_outcome), key=predicted_outcome.tolist().count)\n",
        "          predictions.append(prediction)\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Implement probability estimation for KNN\n",
        "        probabilities = []\n",
        "        for test_point in X.values:\n",
        "            distances = [np.sqrt(np.sum((train_point - test_point)**2)) for train_point in self.X_train.values]\n",
        "            sorted_indices = np.argsort(distances)[:self.k]\n",
        "            predicted_outcome = self.y_train[sorted_indices]\n",
        "\n",
        "            # Calculate probabilities\n",
        "            class_counts = {}\n",
        "\n",
        "            for pred in predicted_outcome:\n",
        "                class_counts[pred] = class_counts.get(pred, 0) + 1\n",
        "\n",
        "            target_probabilities = {class_: count / len(predicted_outcome) for class_, count in class_counts.items()}\n",
        "\n",
        "            # Ensure all classes have a probability entry, even if it's zero\n",
        "            all_classes = set(self.y_train)\n",
        "            for class_ in all_classes:\n",
        "                if class_ not in target_probabilities:\n",
        "                    target_probabilities[class_] = 0.0\n",
        "\n",
        "            probabilities.append(list(target_probabilities.values()))\n",
        "\n",
        "        return np.array(probabilities)\n",
        "\n",
        "# Multilayer Perceptron Classifier\n",
        "class MultilayerPerceptron():\n",
        "    def __init__(self, hidden_layers_sizes = [77, 10, 1], learning_rate = 0.001, epochs = 100):\n",
        "        # Initialize MLP with given network structure\n",
        "        self.hidden_layers_sizes = hidden_layers_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.variables = {'w0': None, 'b0': None, 'w1': None, 'b1': None, 'o1': None, 'o2': None, 'a1': None}\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        # self.input_size = None\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "    def initialise_weights(self):\n",
        "        np.random.seed(1)\n",
        "        input_size = self.X.shape[1]\n",
        "\n",
        "        # Xavier/Glorot initialization for the first hidden layer\n",
        "        self.variables[\"w0\"] = np.random.randn(self.hidden_layers_sizes[0], self.hidden_layers_sizes[1]) * np.sqrt(2 / (input_size + self.hidden_layers_sizes[0]))\n",
        "        self.variables[\"b0\"] = np.random.randn(self.hidden_layers_sizes[1],)\n",
        "\n",
        "        # Xavier/Glorot initialization for the second hidden layer\n",
        "        self.variables[\"w1\"] = np.random.randn(self.hidden_layers_sizes[1], self.hidden_layers_sizes[2]) * np.sqrt(2 / (self.hidden_layers_sizes[0] + self.hidden_layers_sizes[1]))\n",
        "        self.variables[\"b1\"] = np.random.randn(self.hidden_layers_sizes[2],)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Implement training logic for MLP including forward and backward propagation\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        #initialise weights\n",
        "        self.initialise_weights()\n",
        "\n",
        "        #perform forward and backward propagation\n",
        "        for i in range(self.epochs):\n",
        "            self._backward_propagation(self._forward_propagation(X))\n",
        "\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Implement prediction logic for MLP\n",
        "\n",
        "        a1 = self.relu_activation(X.dot(self.variables[\"w0\"]) + self.variables[\"b0\"])\n",
        "        prob_pred = self._sigmoid_func(a1.dot(self.variables[\"w1\"]) + self.variables[\"b1\"])\n",
        "        binary_output = [int(np.round(item[0])) for item in prob_pred.values]\n",
        "        return binary_output\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Implement probability estimation for MLP\n",
        "\n",
        "        a1 = self.relu_activation(X.dot(self.variables[\"w0\"]) + self.variables[\"b0\"])\n",
        "        prob_output = self._sigmoid_func(a1.dot(self.variables[\"w1\"]) + self.variables[\"b1\"])\n",
        "        return prob_output\n",
        "\n",
        "\n",
        "    def _forward_propagation(self,X):\n",
        "        # Implement forward propagation for MLP\n",
        "\n",
        "        o1 = self.variables[\"b0\"] + self.X @ (self.variables[\"w0\"]) #calc weighted sum for first hidden layer\n",
        "        a1 = self.relu_activation(o1) #apply ReLU activation function\n",
        "        o2 = self.variables[\"b1\"] + a1 @ (self.variables[\"w1\"]) #weighted sum for output layer\n",
        "        output = self._sigmoid_func(o2) #apply sigmoid activation function\n",
        "\n",
        "        #store for use in backpropagation\n",
        "        self.variables[\"o1\"] = o1\n",
        "        self.variables[\"a1\"] = a1\n",
        "        self.variables[\"o2\"] = o2\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def _backward_propagation(self, predicted_output):\n",
        "        # Implement backward propagation for MLP\n",
        "\n",
        "        #transformation for calculation of gradient\n",
        "        true_labels = self.y.to_numpy().reshape(predicted_output.shape)\n",
        "\n",
        "        # Compute gradients with respect to the output layer\n",
        "        sigmoid_derivative = predicted_output * (1 - predicted_output)\n",
        "        loss_gradient_output = np.divide((1 - true_labels),np.maximum(1 - predicted_output, 1e-10)) - np.divide(true_labels, np.maximum(predicted_output,1e-10))\n",
        "        output_layer_gradient = sigmoid_derivative * loss_gradient_output\n",
        "\n",
        "        # Compute gradients with respect to the first hidden layer\n",
        "        hidden_layer1_gradient = output_layer_gradient @ self.variables[\"w1\"].T\n",
        "        weight2_gradient = self.variables[\"a1\"].T @ output_layer_gradient\n",
        "        bias2_gradient = np.sum(output_layer_gradient, axis=0)\n",
        "\n",
        "        #update weights and biases\n",
        "        self.variables['w1'] -= weight2_gradient * self.learning_rate\n",
        "        self.variables['b1'] -= bias2_gradient * self.learning_rate\n",
        "\n",
        "        # Compute gradients with respect to the input layer\n",
        "        hidden_layer1_derivative = self.relu_derivative(self.variables['o1']) * hidden_layer1_gradient\n",
        "        weight1_gradient = self.X.T @ (hidden_layer1_derivative)\n",
        "        bias1_gradient = np.sum(hidden_layer1_derivative, axis=0)\n",
        "\n",
        "        # Update the weights and biases\n",
        "        self.variables['w0'] -= weight1_gradient * self.learning_rate\n",
        "        self.variables['b0'] -= bias1_gradient * self.learning_rate\n",
        "\n",
        "        pass\n",
        "\n",
        "    def relu_activation(self, weighted_sum):\n",
        "        return np.maximum(0, weighted_sum)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def _sigmoid_func(self, x):\n",
        "        return 1/(np.exp(-x)+1)\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier(Classifier):\n",
        "    def __init__(self):\n",
        "        self.numerical_features = ['F{}'.format(i) for i in range(1, 18)]\n",
        "        self.binary_features = ['F{}'.format(i) for i in range(18, 78)]\n",
        "        self.alpha = 1  # Laplace smoothing parameter\n",
        "        self.gaussian_params = {}  # Parameters for Gaussian features\n",
        "        self.bernoulli_params = {}  # Parameters for Bernoulli features\n",
        "        self.class_probs = {}  # Class probabilities\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Calculate class probabilities\n",
        "        num_samples = len(y)\n",
        "        num_classes = len(set(y))\n",
        "\n",
        "        for i in set(y):\n",
        "            filter = (y == i)\n",
        "            self.class_probs[i] = (sum(filter) + self.alpha) / (num_samples + num_classes * self.alpha) #laplace smoothing to avoid 0 probabilities\n",
        "\n",
        "            # Fit Gaussian model on Gaussian features\n",
        "            for feature in self.numerical_features:\n",
        "                mean = X.loc[filter, feature].mean()\n",
        "                std = X.loc[filter, feature].std()\n",
        "                self.gaussian_params[(i, feature)] = (mean, std)\n",
        "\n",
        "            # Fit Bernoulli model on Bernoulli features\n",
        "            for feature in self.binary_features:\n",
        "                prob_true = (X.loc[filter, feature].sum() + self.alpha) / (sum(filter) + 2 * self.alpha) #laplace smoothing\n",
        "                self.bernoulli_params[(i, feature)] = prob_true\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "\n",
        "        for nil, instance in X.iterrows():\n",
        "            class_probs = {}\n",
        "\n",
        "            #initialise log prob\n",
        "            for p in self.class_probs:\n",
        "                gaussian_log_prob = 0\n",
        "                bernoulli_log_prob = 0\n",
        "\n",
        "                # Calculate Gaussian log probabilities\n",
        "                for feature in self.numerical_features:\n",
        "                    mean, std = self.gaussian_params[(p, feature)]\n",
        "                    gaussian_log_prob += self.gaussian_log_probability(instance[feature], mean, std)\n",
        "\n",
        "                # Calculate Bernoulli log probabilities\n",
        "                for feature in self.binary_features:\n",
        "                    prob_true = self.bernoulli_params[(p, feature)]\n",
        "                    bernoulli_log_prob += self.bernoulli_log_probability(instance[feature], prob_true)\n",
        "\n",
        "                # Calculate overall log probability for the class\n",
        "                class_probs[p] = np.log(self.class_probs[p]) + gaussian_log_prob + bernoulli_log_prob\n",
        "\n",
        "            # Predict the class with the highest probability\n",
        "            prediction = max(class_probs, key=class_probs.get)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probabilities = []\n",
        "\n",
        "        for nil, instance in X.iterrows():\n",
        "            class_probs = {}\n",
        "\n",
        "            #initialise log prob\n",
        "            for i in self.class_probs:\n",
        "                gaussian_log_prob = 0\n",
        "                bernoulli_log_prob = 0\n",
        "\n",
        "                # Calculate Gaussian log probabilities\n",
        "                for feature in self.numerical_features:\n",
        "                    mean, std = self.gaussian_params[(i, feature)]\n",
        "                    gaussian_log_prob += self.gaussian_log_probability(instance[feature], mean, std)\n",
        "\n",
        "                # Calculate Bernoulli log probabilities\n",
        "                for feature in self.binary_features:\n",
        "                    prob_true = self.bernoulli_params[(i, feature)]\n",
        "                    bernoulli_log_prob += self.bernoulli_log_probability(instance[feature], prob_true)\n",
        "\n",
        "                # Calculate overall log probability for the class\n",
        "                class_probs[i] = np.log(self.class_probs[i]) + gaussian_log_prob + bernoulli_log_prob\n",
        "\n",
        "            # Normalize the probabilities\n",
        "            exp_probs = np.exp(list(class_probs.values()))\n",
        "            probabilities.append(exp_probs / np.sum(exp_probs))\n",
        "\n",
        "        return np.array(probabilities)\n",
        "\n",
        "    def gaussian_log_probability(self, x, mean, std):\n",
        "        exponent = -((x - mean) ** 2) / (2 * (std ** 2))\n",
        "        return exponent + -0.5 * np.log(2 * np.pi * (std ** 2))\n",
        "\n",
        "    def bernoulli_log_probability(self, x, prob_true):\n",
        "        return (1 - x) * np.log(1 - prob_true) + x * np.log(prob_true)\n",
        "\n",
        "\n",
        "\n",
        "# Function to evaluate the performance of the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Predict using the model and calculate various performance metrics\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, zero_division=1)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    mcc = matthews_corrcoef(y_test, predictions)\n",
        "\n",
        "    # Check if the model supports predict_proba method for AUC calculation\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        proba = model.predict_proba(X_test)\n",
        "\n",
        "    #     if isinstance(proba, pd.DataFrame):\n",
        "    #         proba = proba.to_numpy()#         if len(np.unique(y_test)) == 2:  # Binary classification\n",
        "    #         auc = roc_auc_score(y_test, proba[:, 1])\n",
        "    #     else:  # Multiclass classification\n",
        "    #         auc = roc_auc_score(y_test, proba, multi_class='ovo')\n",
        "    # else:\n",
        "    #     auc = None\n",
        "\n",
        "        if len(np.unique(y_test)) == 2:  # Binary classification\n",
        "             # Ensure y_test is binary\n",
        "            if proba.ndim > 1 and proba.shape[1] > 1:\n",
        "                # # Convert y_test to binary format, specifying the positive class label\n",
        "                if len(np.unique(y_test)) > 2:\n",
        "                    y_test = (y_test == positive_class_label).astype(int)  # Specify the positive class label\n",
        "                auc = roc_auc_score(y_test, proba[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_test, proba)\n",
        "\n",
        "\n",
        "        else:  # Multiclass classification\n",
        "            # Ensure y_test is not multilabel\n",
        "            if len(np.unique(y_test)) > 2:\n",
        "                y_test = (y_test == positive_class_label).astype(int)  # Specify the positive class label\n",
        "            auc = roc_auc_score(y_test, proba, multi_class='ovo')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'mcc': mcc,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "# Main function to execute the pipeline\n",
        "def main():\n",
        "    # Load trainWithLable data\n",
        "    df = pd.read_csv('/content/drive/My Drive/ML project/trainWithLabel.csv')\n",
        "\n",
        "    # Preprocess the training data\n",
        "    preprocessor = Preprocessor(df)\n",
        "    df_processed = preprocessor.preprocess()\n",
        "\n",
        "    # Define the models for classification\n",
        "    models = {'Naive Bayes': NaiveBayesClassifier(),\n",
        "              'KNN': KNearestNeighbors(),\n",
        "              'MLP': MultilayerPerceptron()\n",
        "    }\n",
        "\n",
        "    # Split the dataset into features and target variable\n",
        "    X_train = df_processed.drop('Outcome', axis=1)\n",
        "    y_train = df_processed['Outcome']\n",
        "    df_processed['Outcome'] = df_processed['Outcome'].astype('category')\n",
        "\n",
        "    # Reset the index to ensure it starts from 0 and is sequential\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "\n",
        "    # Perform K-Fold cross-validation\n",
        "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "    cv_results = []\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        for fold_idx, (train_index, val_index) in enumerate(kf.split(X_train), start=1):\n",
        "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "            X_train_fold  = X_train_fold.reset_index(drop=True)\n",
        "            X_val_fold  = X_val_fold.reset_index(drop=True)\n",
        "            y_val_fold  = y_val_fold.reset_index(drop=True)\n",
        "            y_train_fold  = y_train_fold.reset_index(drop=True)\n",
        "\n",
        "\n",
        "            model.fit(X_train_fold, y_train_fold)\n",
        "            fold_result = evaluate_model(model, X_val_fold, y_val_fold)\n",
        "            fold_result['model'] = model_name\n",
        "            fold_result['fold'] = fold_idx\n",
        "            cv_results.append(fold_result)\n",
        "\n",
        "\n",
        "    # Convert CV results to a DataFrame and calculate averages\n",
        "    cv_results_df = pd.DataFrame(cv_results)\n",
        "    avg_results = cv_results_df.groupby('model').mean().reset_index()\n",
        "    avg_results['model'] += ' Average'\n",
        "    all_results_df = pd.concat([cv_results_df, avg_results], ignore_index=True)\n",
        "\n",
        "    # Adjust column order and display results\n",
        "    all_results_df = all_results_df[['model', 'accuracy', 'f1', 'precision', 'recall', 'mcc', 'auc']]\n",
        "\n",
        "    print(\"Cross-validation results:\")\n",
        "    print(all_results_df)\n",
        "\n",
        "    # Save results to an Excel file\n",
        "    all_results_df.to_excel('cv_results.xlsx', index=False)\n",
        "    print(\"Cross-validation results with averages saved to cv_results.xlsx\")\n",
        "\n",
        "    # Load the test dataset, assuming you have a test set CSV file without labels\n",
        "    df_ = pd.read_csv('/content/drive/My Drive/ML project/testWithoutLabel.csv')\n",
        "    preprocessor_ = Preprocessor(df_)\n",
        "    X_test = preprocessor_.preprocess()\n",
        "\n",
        "\n",
        "    # Initialize an empty list to store the predictions of each model\n",
        "    predictions = []\n",
        "\n",
        "\n",
        "    # Make predictions with each model\n",
        "    for name, model in models.items():\n",
        "        model_predictions = model.predict(X_test)\n",
        "        predictions.append({\n",
        "            'model': name,\n",
        "            'predictions': model_predictions\n",
        "        })\n",
        "\n",
        "    # Convert the list of predictions into a DataFrame\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "\n",
        "    # Print the predictions\n",
        "    print(\"Model predictions:\")\n",
        "    print(predictions_df)\n",
        "\n",
        "    # Save the predictions to an Excel file\n",
        "    predictions_df.to_csv('test_results.csv', index=False)\n",
        "    print(\"Model predictions saved to test_results.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
